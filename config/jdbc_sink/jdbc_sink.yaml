# JDBC Sink connector는 스키마 정보가 꼭 필요함
# 원본 Record 값이 단순 json이고,  내부에 schema가 없는 경우 => KStreams, ksqlDB, SchemaRegistry, Flink 등 개별 툴을 이용해 schema를 넣어줘야 함
# 다음 옵션은 JDBC Sink 커넥터에서 스키마를 읽기 위해 꼭 필요한 옵션임
# value.converter: org.apache.kafka.connect.json.JsonConverter  # 또는 Avro
# value.converter.schemas.enable: true
# BytesArrayConverter, StringConverter는 빈 값 처리할 때만 사용하면 될 것 같다.

    - name: jdbcsinktest
      common: {}
      connectors:
        # https://docs.confluent.io/kafka-connectors/jdbc/current/sink-connector/sink_config_options.html
        - name: sink-mysql-0
          config:
            connector.class: io.confluent.connect.jdbc.JdbcSinkConnector
            tasks.max: 1
            connection.url: jdbc:mysql://my-mysql:3306/mydb
            connection.user: myuser
            connection.password: mypass

            key.converter: org.apache.kafka.connect.storage.StringConverter  # org.apache.kafka.connect.converters.ByteArrayConverter
            value.converter: org.apache.kafka.connect.json.JsonConverter
            key.converter:.schemas.enable: false
            value.converter.schemas.enable: true

            auto.create: false  # 테이블 자동생성 허용여부. 스키마관리때문에 production에선 false
            auto.evolve: false  # 스키마 업데이트 허용여부. 신규 데이터에서 추가된 컬럼이 있으면 반영해줌. 스키마 관리때문에 production에선 false
            delete.enabled: false  # default # record value가 null이면 삭제취급. pk.mode에서 record_key 사용시에만 사용가능.
            
            insert.mode: insert   # insert, update, upsert   # upsert가 at least once에 좋음. pk.mode pk.fields 필요. 처리 리소스 증가.
            # MySQL 기준
            # insert 모드 도중 중복로그 발생시
              # 대상 테이블에 pk가 있는 경우  => connector 앱 Fail (다른 테이블 sink도 모두 멈추게되므로 주의!!)
              # 대상 테이블에 pk가 없는 경우  => 중복로그 그대로 insert
            # upsert 모드 도중 중복로그 발생시
              # 대상 테이블에 pk가 있는 경우  => 문제없음 (동일한 key의 로그를 override)
              # 대상 테이블에 pk가 없는 경우  => 중복로그 그대로 insert. (table 전체를 스캔하지 않고, 그냥 insert)
            pk.mode: record_value  # none
            pk.fields: logUUID

            table.name.format: "${topic}"
            topics.regex: testdaw

            # 재시도: 일시적 통신오류 등 상황에서 재시도를 의미하며, DB or 테이블이 존재하지 않을 때는 재시도없이 그냥 Fail됨
            max.retries: 300
            retry.backoff.ms: 3000  # 재시도 간격 

            # transforms: HoistField,InsertField,ReplaceField,Cast
            # transforms.HoistField.type: "org.apache.kafka.connect.transforms.HoistField$Value"
            # transforms.HoistField.field: value
            # transforms.InsertField.type: "org.apache.kafka.connect.transforms.InsertField$Value"
            # transforms.InsertField.static.field: __connect_topic
            # transforms.InsertField.static.value: my-topic
            # transforms.ReplaceField.type: "org.apache.kafka.connect.transforms.ReplaceField$Value"
            # transforms.ReplaceField.renames: value:payload
            # transforms.Cast.type: org.apache.kafka.connect.transforms.Cast$Value
            # transforms.Cast.spec: payload:struct

            # tombstones.on.delete: true
            # transforms: ignoreTS
            # transforms.ignoreTS.type: io.confluent.connect.transforms.TombstoneHandler
            # transforms.ignoreTS.behavior”: ignore
            # transforms: ValueToString
            # transforms.ValueToString.type: org.apache.kafka.connect.transforms.ValueToString$Value

            # transforms: HoistField,ExtractField
            # transforms.HoistField.type": org.apache.kafka.connect.transforms.HoistField$Value
            # transforms.HoistField.field": value
            # transforms.ExtractField.type": org.apache.kafka.connect.transforms.ExtractField$Value
            # transforms.ExtractField.field": value